{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run with usloth"
      ],
      "metadata": {
        "id": "ng-UMO6U5loC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "gcBRABYGynSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "L_puTOAWpCXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset_name = \"csebuetnlp/xlsum\"\n",
        "language = \"arabic\"\n",
        "number_of_summarize = 5000\n",
        "\n",
        "min_text_length = 250\n",
        "\n",
        "\n",
        "df_original = load_dataset(dataset_name, language, split=\"train\")\n",
        "df_original = pd.DataFrame(df_original)\n",
        "df_original[\"text_length\"] = df_original[\"text\"].str.len()\n",
        "df_original = df_original[df_original[\"text_length\"] >= min_text_length]\n",
        "df_original = df_original.sort_values(\"text_length\").head(number_of_summarize).drop(columns=[\"text_length\",\"title\",\"summary\",\"url\",\"id\"])\n",
        "df_original.reset_index(drop=True,inplace=True)\n",
        "df_original.to_csv(\"df_original.csv\",index=False)"
      ],
      "metadata": {
        "id": "Nxad5I_hymaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9zwLk9Zr4qHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have used the avialable tutorials in this link: https://github.com/unslothai/unsloth"
      ],
      "metadata": {
        "id": "zYhQtsMYpdvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "    !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ],
      "metadata": {
        "id": "BmJcFBZRK29H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "model_name = \"Qwen/Qwen2.5-14B-Instruct\"\n",
        "max_seq_length = 500  # Balanced for speed and memory on T4\n",
        "\n",
        "# Load model with Unsloth\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=torch.float16,  # T4 excels with FP16 (not BF16)\n",
        "    load_in_4bit=True,    # AWQ 4-bit\n",
        "    device_map=\"auto\"     # Maps to T4 GPU\n",
        ")\n",
        "# Enable inference mode\n",
        "model = FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "id": "sFElt0a9yLPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summaries(csv_path, prompt, batch_size=4,start_index=0, save_interval=100,output_csv=\"summaries.csv\" ):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    total_rows = len(df)\n",
        "    print(f\"Loaded dataset with {total_rows} rows.\")\n",
        "    if os.path.exists(output_csv):\n",
        "        existing_df = pd.read_csv(output_csv)\n",
        "        processed_rows = len(existing_df)\n",
        "        if processed_rows > start_index:\n",
        "            start_index = processed_rows\n",
        "            print(f\"Resuming from {start_index} rows already processed.\")\n",
        "    else:\n",
        "\n",
        "        existing_df = pd.DataFrame(columns=[\"text\", \"summary\"])\n",
        "        existing_df.to_csv(output_csv, index=False)\n",
        "\n",
        "    for i in range(start_index, total_rows, batch_size):\n",
        "        batch_end = min(i + batch_size, total_rows)\n",
        "        batch_df = df.iloc[i:batch_end]\n",
        "        batch_texts = batch_df[\"text\"].tolist()\n",
        "\n",
        "        input_texts = [f\"{prompt}\\nالنص {text} الملخص \" for text in batch_texts]\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            input_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_seq_length,\n",
        "\n",
        "        ).to(\"cuda\")\n",
        "        tokenized_lengths = [len(input_ids) for input_ids in inputs['input_ids']]\n",
        "        max_new_tokens_gen = int(max(tokenized_lengths)/2)\n",
        "        start_time = time.time()\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens_gen,\n",
        "            min_length=5,\n",
        "            use_cache=True,\n",
        "            num_beams=1,\n",
        "            do_sample=False,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        batch_time = time.time() - start_time\n",
        "        summaries = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        batch_results = pd.DataFrame({\n",
        "            \"text\": batch_texts,\n",
        "            \"summary\": summaries\n",
        "        })\n",
        "\n",
        "        batch_results.to_csv(output_csv, mode='a', header=False, index=False)\n",
        "        print(f\"Processed rows {i}–{batch_end}/{total_rows} \"\n",
        "              f\"({batch_time:.2f} s, ~{batch_time/batch_size:.2f} s/row)\")\n",
        "\n",
        "        if (batch_end % save_interval == 0) or (batch_end == total_rows):\n",
        "            print(f\"Checkpoint saved at row {batch_end} to {output_csv}\")\n",
        "\n",
        "    print(f\"Done! All summaries saved to {output_csv}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0WtlsjGA1c0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "csv_path = \"df_original.csv\"\n",
        "prompt = \" اكتب ملخصًا موجزًا وقصيرًا للنص التالي دون إضافة معلومات إضافية أو بيانات وصفية: \"\n",
        "\n",
        "generate_summaries(\n",
        "    csv_path=csv_path,\n",
        "    prompt=prompt,\n",
        "    batch_size=20,\n",
        "    start_index=0,\n",
        "    save_interval=100,\n",
        "    output_csv=\"summaries.csv\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "fgThGo0786Xn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}